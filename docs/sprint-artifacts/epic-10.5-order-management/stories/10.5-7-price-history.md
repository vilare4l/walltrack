# Story 10.5.7: Price History Collection

## Story Info
- **Epic**: Epic 10.5 - Order Management, Price Oracle & Risk-Based Sizing
- **Status**: ready
- **Priority**: P1 - High
- **Story Points**: 5
- **Depends on**: Story 10.5.6 (Price Oracle)

## User Story

**As a** system operator,
**I want** stocker l'historique des prix pour les positions actives,
**So that** le What-If simulator et l'analytics ont des données historiques.

## Acceptance Criteria

### AC 1: Price Collection for Active Positions
**Given** une position est ouverte
**When** le price collector tourne (toutes les 5 secondes)
**Then** le prix est stocké dans price_history
**And** source et timestamp sont enregistrés

### AC 2: Batch Collection Efficiency
**Given** 10 positions actives sur 5 tokens différents
**When** le collector fetch les prix
**Then** il utilise le batch API (1 appel pour 5 tokens)
**And** chaque position reçoit son prix

### AC 3: History Retention
**Given** une position est ouverte depuis 3 jours
**When** je query l'historique
**Then** j'ai les prix des 3 derniers jours
**And** résolution = 5 secondes pendant les 24h récentes
**And** résolution = 1 minute pour les données plus anciennes (compression)

### AC 4: Cleanup After Position Close
**Given** la position est clôturée depuis >7 jours
**When** le cleanup job tourne
**Then** l'historique détaillé est supprimé
**And** un résumé (OHLC daily) est conservé indéfiniment

### AC 5: Peak/Trough Detection
**Given** l'historique existe pour une position
**When** je demande les métriques
**Then** peak_price (plus haut depuis entrée) est calculé
**And** max_drawdown depuis peak est disponible
**And** unrealized_pnl_peak est trackée

## Technical Specifications

### Database Schema

**migrations/V10__price_history.sql:**
```sql
-- Price history for positions
-- Partitioned by date for efficient cleanup

CREATE TABLE IF NOT EXISTS walltrack.price_history (
    id BIGSERIAL,
    position_id UUID NOT NULL,
    token_address VARCHAR(44) NOT NULL,

    -- Price data
    price DECIMAL(30, 12) NOT NULL,
    source VARCHAR(20) NOT NULL,

    -- Timestamp
    recorded_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    -- Partitioning
    PRIMARY KEY (id, recorded_at)
) PARTITION BY RANGE (recorded_at);

-- Create partitions for current and next month
CREATE TABLE IF NOT EXISTS walltrack.price_history_current PARTITION OF walltrack.price_history
    FOR VALUES FROM (DATE_TRUNC('month', NOW())) TO (DATE_TRUNC('month', NOW()) + INTERVAL '1 month');

CREATE TABLE IF NOT EXISTS walltrack.price_history_next PARTITION OF walltrack.price_history
    FOR VALUES FROM (DATE_TRUNC('month', NOW()) + INTERVAL '1 month') TO (DATE_TRUNC('month', NOW()) + INTERVAL '2 months');

-- Indexes
CREATE INDEX IF NOT EXISTS idx_price_history_position ON walltrack.price_history(position_id, recorded_at DESC);
CREATE INDEX IF NOT EXISTS idx_price_history_token ON walltrack.price_history(token_address, recorded_at DESC);

-- Compressed price history (1-minute OHLC)
CREATE TABLE IF NOT EXISTS walltrack.price_history_compressed (
    id BIGSERIAL PRIMARY KEY,
    position_id UUID NOT NULL,
    token_address VARCHAR(44) NOT NULL,

    -- OHLC data
    period_start TIMESTAMPTZ NOT NULL,
    open_price DECIMAL(30, 12) NOT NULL,
    high_price DECIMAL(30, 12) NOT NULL,
    low_price DECIMAL(30, 12) NOT NULL,
    close_price DECIMAL(30, 12) NOT NULL,

    -- Sample count
    sample_count INTEGER NOT NULL DEFAULT 1,

    UNIQUE(position_id, period_start)
);

CREATE INDEX IF NOT EXISTS idx_price_compressed_position ON walltrack.price_history_compressed(position_id, period_start DESC);

-- Position price metrics (materialized for performance)
CREATE TABLE IF NOT EXISTS walltrack.position_price_metrics (
    position_id UUID PRIMARY KEY,

    -- Peak tracking
    peak_price DECIMAL(30, 12) NOT NULL,
    peak_at TIMESTAMPTZ NOT NULL,

    -- Current metrics
    current_price DECIMAL(30, 12),
    last_update TIMESTAMPTZ,

    -- Drawdown from peak
    max_drawdown_pct DECIMAL(10, 4) NOT NULL DEFAULT 0,
    current_drawdown_pct DECIMAL(10, 4) NOT NULL DEFAULT 0,

    -- PnL at peak
    unrealized_pnl_at_peak DECIMAL(20, 8),

    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

### Price Collector Service

**src/walltrack/services/pricing/price_collector.py:**
```python
"""Collects and stores price history for positions."""

import asyncio
from datetime import datetime, timedelta
from decimal import Decimal
from typing import Optional

import structlog

from walltrack.services.pricing.price_oracle import PriceOracle, get_price_oracle
from walltrack.data.supabase.repositories.position_repo import PositionRepository
from walltrack.models.position import Position, PositionStatus

logger = structlog.get_logger(__name__)


class PriceCollector:
    """
    Collects price data for active positions.

    Runs periodically to store price history.
    """

    def __init__(
        self,
        price_oracle: PriceOracle,
        position_repo: PositionRepository,
        collection_interval: float = 5.0,
    ):
        self.oracle = price_oracle
        self.position_repo = position_repo
        self.interval = collection_interval
        self._running = False
        self._task: Optional[asyncio.Task] = None

    async def start(self) -> None:
        """Start the price collector."""
        if self._running:
            return

        self._running = True
        self._task = asyncio.create_task(self._collection_loop())
        logger.info("price_collector_started", interval=self.interval)

    async def stop(self) -> None:
        """Stop the price collector."""
        self._running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("price_collector_stopped")

    async def _collection_loop(self) -> None:
        """Main collection loop."""
        while self._running:
            try:
                await self._collect_prices()
            except Exception as e:
                logger.error("price_collection_error", error=str(e))

            await asyncio.sleep(self.interval)

    async def _collect_prices(self) -> None:
        """Collect prices for all active positions."""
        # Get active positions
        positions = await self.position_repo.get_active_positions()

        if not positions:
            return

        # Group by token for batch fetching
        token_positions: dict[str, list[Position]] = {}
        for pos in positions:
            if pos.token_address not in token_positions:
                token_positions[pos.token_address] = []
            token_positions[pos.token_address].append(pos)

        # Batch fetch prices
        token_addresses = list(token_positions.keys())
        price_results = await self.oracle.get_prices_batch(token_addresses)

        # Store prices and update metrics
        now = datetime.utcnow()

        for token_address, positions in token_positions.items():
            result = price_results.get(token_address)

            if result and result.success:
                for position in positions:
                    await self._store_price(position, result.price, result.source.value, now)
                    await self._update_metrics(position, result.price, now)

        logger.debug(
            "prices_collected",
            positions=len(positions),
            tokens=len(token_addresses)
        )

    async def _store_price(
        self,
        position: Position,
        price: Decimal,
        source: str,
        timestamp: datetime
    ) -> None:
        """Store price point in history."""
        from walltrack.data.supabase.client import get_supabase_client

        client = await get_supabase_client()

        await client.table("price_history").insert({
            "position_id": str(position.id),
            "token_address": position.token_address,
            "price": str(price),
            "source": source,
            "recorded_at": timestamp.isoformat(),
        }).execute()

    async def _update_metrics(
        self,
        position: Position,
        current_price: Decimal,
        timestamp: datetime
    ) -> None:
        """Update position price metrics."""
        from walltrack.data.supabase.client import get_supabase_client

        client = await get_supabase_client()

        # Get current metrics
        result = await client.table("position_price_metrics").select("*") \
            .eq("position_id", str(position.id)) \
            .single() \
            .execute()

        if result.data:
            metrics = result.data
            peak_price = Decimal(str(metrics["peak_price"]))

            # Update peak if new high
            if current_price > peak_price:
                peak_price = current_price
                unrealized_pnl = (current_price - position.entry_price) * position.entry_amount_tokens

                await client.table("position_price_metrics").update({
                    "peak_price": str(peak_price),
                    "peak_at": timestamp.isoformat(),
                    "current_price": str(current_price),
                    "last_update": timestamp.isoformat(),
                    "current_drawdown_pct": "0",
                    "unrealized_pnl_at_peak": str(unrealized_pnl),
                    "updated_at": timestamp.isoformat(),
                }).eq("position_id", str(position.id)).execute()
            else:
                # Calculate drawdown from peak
                drawdown_pct = ((peak_price - current_price) / peak_price) * 100
                max_drawdown = max(
                    Decimal(str(metrics["max_drawdown_pct"])),
                    drawdown_pct
                )

                await client.table("position_price_metrics").update({
                    "current_price": str(current_price),
                    "last_update": timestamp.isoformat(),
                    "current_drawdown_pct": str(drawdown_pct),
                    "max_drawdown_pct": str(max_drawdown),
                    "updated_at": timestamp.isoformat(),
                }).eq("position_id", str(position.id)).execute()

        else:
            # Create initial metrics
            await client.table("position_price_metrics").insert({
                "position_id": str(position.id),
                "peak_price": str(current_price),
                "peak_at": timestamp.isoformat(),
                "current_price": str(current_price),
                "last_update": timestamp.isoformat(),
            }).execute()


class PriceHistoryCompressor:
    """Compresses old price history to OHLC format."""

    async def compress_old_data(self, older_than_hours: int = 24) -> int:
        """Compress price history older than threshold."""
        from walltrack.data.supabase.client import get_supabase_client

        client = await get_supabase_client()
        cutoff = datetime.utcnow() - timedelta(hours=older_than_hours)

        # Get distinct position/minute combinations to compress
        compressed_count = 0

        # This would be a more complex aggregation query in practice
        # Simplified here for the story

        logger.info("price_history_compressed", count=compressed_count)
        return compressed_count


class PriceHistoryCleanup:
    """Cleans up old price history for closed positions."""

    async def cleanup_closed_positions(self, closed_days_ago: int = 7) -> int:
        """Delete detailed history for long-closed positions."""
        from walltrack.data.supabase.client import get_supabase_client

        client = await get_supabase_client()
        cutoff = datetime.utcnow() - timedelta(days=closed_days_ago)

        # Get positions closed before cutoff
        result = await client.table("positions").select("id") \
            .eq("status", "closed") \
            .lt("closed_at", cutoff.isoformat()) \
            .execute()

        deleted_count = 0
        for row in result.data:
            position_id = row["id"]

            # Delete from price_history (keep compressed)
            await client.table("price_history") \
                .delete() \
                .eq("position_id", position_id) \
                .execute()

            deleted_count += 1

        logger.info("price_history_cleaned", positions=deleted_count)
        return deleted_count
```

### Price History Repository

**src/walltrack/data/supabase/repositories/price_history_repo.py:**
```python
"""Repository for price history queries."""

from datetime import datetime
from decimal import Decimal
from typing import Optional
from uuid import UUID

from pydantic import BaseModel


class PricePoint(BaseModel):
    """Single price point."""
    price: Decimal
    source: str
    recorded_at: datetime


class PositionPriceMetrics(BaseModel):
    """Price metrics for a position."""
    peak_price: Decimal
    peak_at: datetime
    current_price: Optional[Decimal]
    max_drawdown_pct: Decimal
    current_drawdown_pct: Decimal
    unrealized_pnl_at_peak: Optional[Decimal]


class PriceHistoryRepository:
    """Repository for price history queries."""

    async def get_history(
        self,
        position_id: UUID,
        start: Optional[datetime] = None,
        end: Optional[datetime] = None,
        limit: int = 1000,
    ) -> list[PricePoint]:
        """Get price history for a position."""
        from walltrack.data.supabase.client import get_supabase_client

        client = await get_supabase_client()

        query = client.table("price_history").select("*") \
            .eq("position_id", str(position_id))

        if start:
            query = query.gte("recorded_at", start.isoformat())
        if end:
            query = query.lte("recorded_at", end.isoformat())

        result = await query.order("recorded_at", desc=True).limit(limit).execute()

        return [PricePoint(**row) for row in result.data]

    async def get_metrics(self, position_id: UUID) -> Optional[PositionPriceMetrics]:
        """Get price metrics for a position."""
        from walltrack.data.supabase.client import get_supabase_client

        client = await get_supabase_client()

        result = await client.table("position_price_metrics").select("*") \
            .eq("position_id", str(position_id)) \
            .single() \
            .execute()

        if result.data:
            return PositionPriceMetrics(**result.data)
        return None
```

## Implementation Tasks

- [ ] Create V10 migration for price_history tables
- [ ] Create partitioned table structure
- [ ] Create compressed history table
- [ ] Create position metrics table
- [ ] Implement PriceCollector service
- [ ] Implement batch collection
- [ ] Implement peak/trough tracking
- [ ] Implement PriceHistoryCompressor
- [ ] Implement PriceHistoryCleanup
- [ ] Create PriceHistoryRepository
- [ ] Add scheduler jobs for compression/cleanup
- [ ] Write tests

## Definition of Done

- [ ] Prices collected every 5 seconds for active positions
- [ ] Batch API calls minimize external requests
- [ ] Peak/drawdown metrics updated in real-time
- [ ] Old data compressed to 1-minute OHLC
- [ ] Closed position data cleaned after 7 days
- [ ] Full test coverage

## File List

### New Files
- `migrations/V10__price_history.sql` - Price history tables
- `src/walltrack/services/pricing/price_collector.py` - Collector service
- `src/walltrack/services/pricing/compressor.py` - History compressor
- `src/walltrack/data/supabase/repositories/price_history_repo.py` - Repository
- `tests/unit/services/pricing/test_price_collector.py` - Tests
